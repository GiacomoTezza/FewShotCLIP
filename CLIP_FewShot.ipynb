{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7ImPOvighMz"
   },
   "source": [
    "# Few-Shot Adaptation of CLIP with CoCoOp for Fine-Grained Flower Classification\n",
    "**Deep Learning Project Assignment 2025**<br>\n",
    "**Team**: Mayora Barcenas Valeria, Tomelleri Jacopo, Tezza Giacomo.\n",
    "\n",
    "## Abstract\n",
    "2‚Äì3 sentences summarizing purpose and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StQmJNoLghM0"
   },
   "source": [
    "## Introduction & Motivation\n",
    "\n",
    "##### TODO\n",
    "- Briefly introduce CLIP, few-shot learning, and fine-grained visual recognition.\n",
    "- Introduce the Oxford Flowers dataset and its challenges (subtle inter-class differences, few labeled examples).\n",
    "- State the goal: improving CLIP's zero-shot performance via few-shot adaptation with CoCoOp.\n",
    "- Motivate CoCoOp as the starting method (mention previous results/generalization).\n",
    "- Define your plan: implement CoCoOp ‚Üí baseline ‚Üí extend ‚Üí evaluate ‚Üí propose future directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgX1ILR7ghM0"
   },
   "source": [
    "## Setup and Baseline\n",
    "##### TODO\n",
    "- Environment setup: pip install (all dependencies including clip, torch, tqdm, matplotlib, etc.)\n",
    "- Set seeds for reproducibility\n",
    "- Load the Oxford Flowers dataset (reuse template.py's get_data)\n",
    "    - Show Base/Novel split and explain its purpose (simulate generalization)\n",
    "- Load CLIP (ViT-B/16) and inspect its architecture (input size, vocab, etc.)\n",
    "- Perform CLIP zero-shot evaluation:\n",
    "    - Generate prompts: ‚Äúa photo of a {flower}, a type of flower‚Äù\n",
    "    - Evaluate and report base, novel, and harmonic mean accuracies.\n",
    "- Plot: per-class accuracy bar chart, confusion matrix\n",
    "- Save these metrics for later comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwTekigDghM0"
   },
   "source": [
    "### Dependencies and Environment Setup\n",
    "This section ensures the notebook is reproducible and fully operational across different environments, including Google Colab, AWS SageMaker, and local machines.<br/>\n",
    "Here it will install all necessary packages, set the working device (CPU/GPU), and configure paths and reproducibility settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VopeGTnSghM1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
    "\n",
    "install(\"ftfy\")\n",
    "install(\"regex\")\n",
    "install(\"tqdm\")\n",
    "install(\"scikit-learn\")\n",
    "install(\"scikit-image\")\n",
    "install(\"pooch\")\n",
    "install(\"matplotlib\")\n",
    "install(\"pillow\")\n",
    "install(\"openai-clip\")\n",
    "install(\"torch>=2.0\")\n",
    "install(\"torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtqdSOr8qqOn",
    "outputId": "188080f0-3d02-4399-d990-65997a1eeb6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 14:36:53.041401: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-10 14:36:53.055256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752158213.073409    2766 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752158213.079110    2766 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-10 14:36:53.097088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "import clip  # OpenAI CLIP\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "arDGJv6xghM2"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M9V0U92FghM2"
   },
   "outputs": [],
   "source": [
    "# For saving checkpoints, plots, etc.\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "Path(\"data\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2353MHw1p24h"
   },
   "source": [
    "### Dataset Loading and Split\n",
    "Downloading the data directly from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M_1CrUhZpVCq"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJI_a5EizA5a"
   },
   "source": [
    "#### Base and Novel categories\n",
    "The Oxford Flowers dataset contains 102 classes of flowers. For our experiments, we will split the dataset into base and novel categories. The first 51 classes will be used as base categories, while the remaining 51 classes will be treated as novel categories. This split allows us to simulate a real-world scenario where the model is trained on a set of known categories (base) and evaluated on unseen categories (novel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nfq51vd8q_5a"
   },
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvDdoYQr2fIu"
   },
   "source": [
    "#### Inspect Classes\n",
    "To inspect the classes, we will first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we will split it using `base_novel_categories`. Finally, we will use the hard-coded `CLASS_NAMES` to print the class names in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veGGpNDctCgR",
    "outputId": "3a95f8a3-5878-41e1-d8d8-ba21390562ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Classes:\n",
      "  0: pink primrose\n",
      "  1: hard-leaved pocket orchid\n",
      "  2: canterbury bells\n",
      "  3: sweet pea\n",
      "  4: english marigold\n",
      "  5: tiger lily\n",
      "  6: moon orchid\n",
      "  7: bird of paradise\n",
      "  8: monkshood\n",
      "  9: globe thistle\n",
      " 10: snapdragon\n",
      " 11: colt's foot\n",
      " 12: king protea\n",
      " 13: spear thistle\n",
      " 14: yellow iris\n",
      " 15: globe-flower\n",
      " 16: purple coneflower\n",
      " 17: peruvian lily\n",
      " 18: balloon flower\n",
      " 19: giant white arum lily\n",
      " 20: fire lily\n",
      " 21: pincushion flower\n",
      " 22: fritillary\n",
      " 23: red ginger\n",
      " 24: grape hyacinth\n",
      " 25: corn poppy\n",
      " 26: prince of wales feathers\n",
      " 27: stemless gentian\n",
      " 28: artichoke\n",
      " 29: sweet william\n",
      " 30: carnation\n",
      " 31: garden phlox\n",
      " 32: love in the mist\n",
      " 33: mexican aster\n",
      " 34: alpine sea holly\n",
      " 35: ruby-lipped cattleya\n",
      " 36: cape flower\n",
      " 37: great masterwort\n",
      " 38: siam tulip\n",
      " 39: lenten rose\n",
      " 40: barbeton daisy\n",
      " 41: daffodil\n",
      " 42: sword lily\n",
      " 43: poinsettia\n",
      " 44: bolero deep blue\n",
      " 45: wallflower\n",
      " 46: marigold\n",
      " 47: buttercup\n",
      " 48: oxeye daisy\n",
      " 49: common dandelion\n",
      " 50: petunia\n",
      "\n",
      "Novel Classes:\n",
      " 51: wild pansy\n",
      " 52: primula\n",
      " 53: sunflower\n",
      " 54: pelargonium\n",
      " 55: bishop of llandaff\n",
      " 56: gaura\n",
      " 57: geranium\n",
      " 58: orange dahlia\n",
      " 59: pink-yellow dahlia?\n",
      " 60: cautleya spicata\n",
      " 61: japanese anemone\n",
      " 62: black-eyed susan\n",
      " 63: silverbush\n",
      " 64: californian poppy\n",
      " 65: osteospermum\n",
      " 66: spring crocus\n",
      " 67: bearded iris\n",
      " 68: windflower\n",
      " 69: tree poppy\n",
      " 70: gazania\n",
      " 71: azalea\n",
      " 72: water lily\n",
      " 73: rose\n",
      " 74: thorn apple\n",
      " 75: morning glory\n",
      " 76: passion flower\n",
      " 77: lotus\n",
      " 78: toad lily\n",
      " 79: anthurium\n",
      " 80: frangipani\n",
      " 81: clematis\n",
      " 82: hibiscus\n",
      " 83: columbine\n",
      " 84: desert-rose\n",
      " 85: tree mallow\n",
      " 86: magnolia\n",
      " 87: cyclamen\n",
      " 88: watercress\n",
      " 89: canna lily\n",
      " 90: hippeastrum\n",
      " 91: bee balm\n",
      " 92: ball moss\n",
      " 93: foxglove\n",
      " 94: bougainvillea\n",
      " 95: camellia\n",
      " 96: mallow\n",
      " 97: mexican petunia\n",
      " 98: bromelia\n",
      " 99: blanket flower\n",
      "100: trumpet creeper\n",
      "101: blackberry lily\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Pretty formatted print of base and novel classes\n",
    "def print_classes(label, classes, class_names):\n",
    "    print(f\"{label} Classes:\")\n",
    "    for i in classes:\n",
    "        print(f\"{i:3d}: {class_names[i]}\")\n",
    "    print()\n",
    "\n",
    "print_classes(\"Base\", base_classes, CLASS_NAMES)\n",
    "print_classes(\"Novel\", novel_classes, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puO1VNpzwvi"
   },
   "source": [
    "#### Split Dataset\n",
    "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
    "To split the data we need the dataset and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "msOszMs2zRRu"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, base_classes):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQZT22rE8hBw"
   },
   "source": [
    "#### Extract k shots\n",
    "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
    "Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KpbPRLr7WL_"
   },
   "source": [
    "### CLIP (ViT-B/16) Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sh6uLZRT7YJx",
    "outputId": "e695fe2f-32b0-419e-b8c4-de5a8eadcacf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f0280968c20>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM9H14899ses"
   },
   "source": [
    "#### Load and Prepare Data\n",
    "Here we get the three dataset split and pass clip pre-defined augmentations.\n",
    "Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
    "Finally, we split the three datasets into base and novel categories.\n",
    "As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TVrYUYTv9ttM"
   },
   "outputs": [],
   "source": [
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcgMwr3J9VIg"
   },
   "source": [
    "### Compute Zero-Shot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "7uhblkvm9US4",
    "outputId": "dd9a232f-37a7-4229-e133-6a27661cda3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Zero-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.85it/s]\n",
      "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:16<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Base classes accuracy: 71.33%\n",
      "üîç Novel classes accuracy: 78.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(model, dataset, categories, batch_size, device, label=\"\"):\n",
    "    # let's set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, while novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baYfLKNdfbUR"
   },
   "source": [
    "#### Harmonic Mean\n",
    "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
    "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
    "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rKAXR7hlfbUR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Harmonic Mean: 74.62%\n"
     ]
    }
   ],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "print(f\"üîç Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTfBAJovghM6"
   },
   "source": [
    "### Baseline evaluation and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trHBv5NEghM6"
   },
   "source": [
    "## Methodology: CoCoOp\n",
    "##### TODO\n",
    "- Provide a short literature summary of CoCoOp with diagram (can be ASCII or markdown)\n",
    "    - What it is (image-conditioned prompt learning)\n",
    "    - Why it helps (avoids overfitting class tokens, learns better generalization)\n",
    "- Describe the architecture:\n",
    "    - MetaNet / PromptLearner: learn image-conditioned soft prompts\n",
    "    - Prompt format: [CLS] + ctx_tokens + class name + EOS\n",
    "    - Only prompt tokens and MetaNet are updated, CLIP remains frozen\n",
    "- Present equations:\n",
    "    - Prompt generation\n",
    "    - Similarity computation\n",
    "    - Cross-entropy loss over cosine similarity\n",
    "- Show a schematic of the training pipeline (optional visual diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwV3pf7YghM6"
   },
   "source": [
    "## Implementation: CoCoOp Baseline\n",
    "In this section, we implement the CoCoOp (Conditional Context Optimization) method for adapting CLIP to our downstream task. CoCoOp extends the original CoOp approach by generating prompts that are conditioned on the input image. The only trainable components are:\n",
    "\n",
    "- A PromptLearner module that generates learnable context tokens\n",
    "- A small MetaNet (image-conditional MLP) that produces dynamic prompt embeddings\n",
    "\n",
    "We freeze CLIP's visual and text encoders and only train the prompt learner to minimize cross-entropy over the Base training set.\n",
    "\n",
    "##### TODO\n",
    "- Create PromptLearner module (copy from lab.py and adapt for Oxford Flowers)\n",
    "- Load CLIP\n",
    "- Freeze CLIP weights\n",
    "- Setup train/val/test splits from base classes only\n",
    "- Train PromptLearner on base classes\n",
    "- Use tokenizer and pre-tokenized text prompts for classnames\n",
    "- Implement optimizer, scheduler, and loss function\n",
    "- Report model summary (trainable params)\n",
    "- Show training logs: loss and accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ThDIpaP7ghM6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "_tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pCo-rRO2ghM6"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts.to(self.dtype) + self.positional_embedding.to(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "        return x @ self.text_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7f2JfmGbghM7"
   },
   "outputs": [],
   "source": [
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, class_ids, class_names, clip_model, n_ctx=4, ctx_init=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        self.device = device\n",
    "        self.n_cls = len(class_ids)\n",
    "        self.n_ctx = n_ctx\n",
    "\n",
    "        self.class_ids = class_ids\n",
    "        self.classnames = [class_names[c].replace(\"_\", \" \") for c in class_ids]\n",
    "\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            # prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).to(self.dtype)\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"# of context tokens: {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ])).to(device)\n",
    "\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in self.classnames]\n",
    "        # tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
    "        tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized).to(self.dtype)\n",
    "\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :]) #SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :]) #CLS, EOS\n",
    "        self.tokenized_prompts = tokenized\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "        return torch.cat([prefix, ctx, suffix], dim=1)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        B = image_features.shape[0]\n",
    "        ctx = self.ctx.to(self.dtype).unsqueeze(0)                # (1, n_ctx, dim)\n",
    "        bias = self.meta_net(image_features.float()).unsqueeze(1)  # (B, 1, dim)\n",
    "        ctx_shifted = ctx + bias                   # (B, n_ctx, dim)\n",
    "\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "\n",
    "        prompts = []\n",
    "        for ctx_i in ctx_shifted:\n",
    "            ctx_exp = ctx_i.unsqueeze(0).expand(self.n_cls, -1, -1)       # (n_cls, n_ctx, dim)\n",
    "            pts = self.construct_prompts(ctx_exp, prefix, suffix)         # (n_cls, ?, dim)\n",
    "            prompts.append(pts)\n",
    "        return torch.stack(prompts)  # (B, n_cls, ?, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RCAoQDpgghM7"
   },
   "outputs": [],
   "source": [
    "class CoCoOp(nn.Module):\n",
    "    def __init__(self, class_ids, class_names, clip_model, n_ctx=4, ctx_init=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.prompt_learner = PromptLearner(class_ids, class_names, clip_model, n_ctx=n_ctx, ctx_init=ctx_init, device=device).to(device)\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model).to(device)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.class_ids = class_ids\n",
    "\n",
    "        # Freeze everything except prompt learner\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"prompt_learner\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        print(f\"Totla parameters: {sum(p.numel() for p in self.parameters())}\")\n",
    "        print(f\"Trainable parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    def encode_text(self, text_inputs):\n",
    "        ctx = self.prompt_learner.ctx # (n_ctx, dim)\n",
    "        prefix = self.prompt_learner.token_prefix # (n_cls, 1, dim)\n",
    "        suffix = self.prompt_learner.token_suffix # (n_cls, ?, dim)\n",
    "        # Build prompts using learned ctx (no conditioning)\n",
    "        ctx = ctx.unsqueeze(0).expand(self.prompt_learner.n_cls, -1, -1)  # (n_cls, n_ctx, dim)\n",
    "        prompts = torch.cat([prefix, ctx, suffix], dim=1)  # (n_cls, prompt_len, dim)\n",
    "        \n",
    "        return self.text_encoder(prompts, self.prompt_learner.tokenized_prompts)\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        image_features = self.image_encoder(image.to(dtype=self.dtype))\n",
    "        return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, images, labels=None):\n",
    "        images = images.to(self.device).to(self.dtype)\n",
    "        image_features = self.image_encoder(images)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "\n",
    "        logits = []\n",
    "        for p_i, i_f in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(p_i, self.tokenized_prompts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            logit = self.logit_scale.exp() * i_f @ text_features.T\n",
    "            logits.append(logit)\n",
    "        logits = torch.stack(logits)\n",
    "\n",
    "        if self.training and labels is not None:\n",
    "            return F.cross_entropy(logits, labels)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdwqkdDYghM7"
   },
   "source": [
    "We now define the training and evaluation logic for CoCoOp using the Base training classes only. The model is trained to optimize cross-entropy loss over class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WrbeKo-9wJYJ"
   },
   "outputs": [],
   "source": [
    "def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  pbar = tqdm(data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader))\n",
    "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    outputs = net(inputs)\n",
    "    loss = cost_function(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    samples += inputs.shape[0]\n",
    "    cumulative_loss += loss.item()\n",
    "    _, predicted = outputs.max(dim=1)\n",
    "\n",
    "    cumulative_accuracy += (predicted == targets).sum().item()\n",
    "\n",
    "    pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
    "    pbar.update(1)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  return cumulative_loss / samples, cumulative_accuracy /samples * 100\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
    "  samples = 0.0\n",
    "  cumulative_loss = 0.0\n",
    "  cumulative_accuracy = 0.0\n",
    "\n",
    "  net.eval()\n",
    "\n",
    "  pbar = tqdm(data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader))\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = net(inputs)\n",
    "      loss = cost_function(outputs, targets)\n",
    "      samples += inputs.shape[0]\n",
    "      cumulative_loss += loss.item()\n",
    "      _, predicted = outputs.max(dim=1)\n",
    "\n",
    "      cumulative_accuracy += (predicted == targets).sum().item()\n",
    "\n",
    "      pbar.set_postfix(test_loss=loss.item(), test_acc=cumulative_accuracy / samples * 100)\n",
    "      pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HM-I2NzmwJXA"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr, wd, momentum):\n",
    "  optimizer = torch.optim.SGD(\n",
    "      [{\"params\": model.parameters()}],\n",
    "      lr=lr,\n",
    "      momentum=momentum,\n",
    "      weight_decay=wd,\n",
    "      nesterov=True,\n",
    "  )\n",
    "  return optimizer\n",
    "\n",
    "def get_cost_function():\n",
    "  cost_function = torch.nn.CrossEntropyLoss()\n",
    "  return cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "9wF9GErBghM7",
    "outputId": "d360277b-c66d-4aa8-aa4c-28fbcffb89ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 51 classes\n",
      "Initial context: \"X X X X\"\n",
      "# of context tokens: 4\n",
      "Totla parameters: 124359201\n",
      "Trainable parameters: 35360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:51<00:00,  2.47it/s, train_acc=67.8, train_loss=0.432] \n",
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:16<00:00,  7.65it/s, test_acc=80.8, test_loss=0.448]  \n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:51<00:00,  2.46it/s, train_acc=80.4, train_loss=2.85] \n",
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:16<00:00,  7.63it/s, test_acc=87.3, test_loss=0.161]  \n",
      "Test Zero Shot on Novel: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:15<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Base classes accuracy: 71.33%\n",
      "üîç Novel classes accuracy: 78.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main_cocoop(\n",
    "    batch_size=4,\n",
    "    device=\"cuda:0\",\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp_1\",\n",
    "    n_ctx=4,\n",
    "    ctx_init=None,\n",
    "):\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "    train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "    base_classes, novel_classes = base_novel_categories(train_set)\n",
    "    train_base, _ = split_data(train_set, base_classes)\n",
    "    val_base, _ = split_data(val_set, base_classes)\n",
    "    test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "    print(f\"Training on {len(base_classes)} classes\")\n",
    "\n",
    "    train_loader = DataLoader(train_base, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_base, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader_base = DataLoader(test_base, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader_novel = DataLoader(test_novel, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = CoCoOp(base_classes, CLASS_NAMES, clip_model, n_ctx=n_ctx, ctx_init=ctx_init, device=device)\n",
    "    optimizer = get_optimizer(model, learning_rate, weight_decay, momentum)\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_acc = training_step(model, train_loader, optimizer, cost_function, device=device)\n",
    "        val_loss, val_acc = test_step(model, val_loader, cost_function, device=device)\n",
    "\n",
    "    eval_novel_acc = eval(model, test_novel, novel_classes, 64, device, label=\"Test Zero Shot on Novel\")\n",
    "\n",
    "    print()\n",
    "    print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "    print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
    "\n",
    "main_cocoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5HXX1FKghM8"
   },
   "source": [
    "## Evaluation: CoCoOp Baseline\n",
    "##### TODO\n",
    "- Evaluate CoCoOp on:\n",
    "    - Base test set (base classes)\n",
    "    - Novel test set (novel classes) ‚Äî zero-shot generalization\n",
    "- Compute Harmonic Mean\n",
    "- Compare with baseline CLIP (from Section 2)\n",
    "- Show:\n",
    "    - Table with Base, Novel, HM for both CLIP and CoCoOp\n",
    "    - Confusion matrices\n",
    "    - Accuracy per class (bar plots)\n",
    "    - Line plot of train/val curves\n",
    "- Discuss: Where does CoCoOp help most? Are any flowers still confused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ_zpWRkghM8"
   },
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJP0e3fwghM8"
   },
   "source": [
    "## Conclusion & Takeaways\n",
    "##### TODO\n",
    "- Summarize performance gains vs. zero-shot\n",
    "- Highlight where CoCoOp shines and its limitations\n",
    "- Discuss extensibility and future work\n",
    "- Reflect on challenges in fine-grained adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHvnymPyghM8"
   },
   "source": [
    "## References\n",
    "##### TODO\n",
    "- Radford et al., 2021 (CLIP)\n",
    "- Zhou et al., 2022 (CoOp/CoCoOp)\n",
    "- Additional relevant few-shot learning work (e.g., Tip-Adapter, VPT, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3cLVIqMghM8"
   },
   "source": [
    "## Appendix"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
