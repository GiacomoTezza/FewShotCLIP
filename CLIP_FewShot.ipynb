{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "b7ImPOvighMz"
			},
			"source": [
				"# Few-Shot Adaptation of CLIP with CoCoOp for Fine-Grained Flower Classification\n",
				"**Deep Learning Project Assignment 2025**<br>\n",
				"**Team**: Mayora Barcenas Valeria, Tomelleri Jacopo, Tezza Giacomo.\n",
				"\n",
				"## Abstract\n",
				"2–3 sentences summarizing purpose and results"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "StQmJNoLghM0"
			},
			"source": [
				"## Introduction & Motivation\n",
				"\n",
				"##### TODO\n",
				"- Briefly introduce CLIP, few-shot learning, and fine-grained visual recognition.\n",
				"- Introduce the Oxford Flowers dataset and its challenges (subtle inter-class differences, few labeled examples).\n",
				"- State the goal: improving CLIP's zero-shot performance via few-shot adaptation with CoCoOp.\n",
				"- Motivate CoCoOp as the starting method (mention previous results/generalization).\n",
				"- Define your plan: implement CoCoOp → baseline → extend → evaluate → propose future directions."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "JgX1ILR7ghM0"
			},
			"source": [
				"## Setup and Baseline\n",
				"##### TODO\n",
				"- Environment setup: pip install (all dependencies including clip, torch, tqdm, matplotlib, etc.)\n",
				"- Set seeds for reproducibility\n",
				"- Load the Oxford Flowers dataset (reuse template.py's get_data)\n",
				"    - Show Base/Novel split and explain its purpose (simulate generalization)\n",
				"- Load CLIP (ViT-B/16) and inspect its architecture (input size, vocab, etc.)\n",
				"- Perform CLIP zero-shot evaluation:\n",
				"    - Generate prompts: “a photo of a {flower}, a type of flower”\n",
				"    - Evaluate and report base, novel, and harmonic mean accuracies.\n",
				"- Plot: per-class accuracy bar chart, confusion matrix\n",
				"- Save these metrics for later comparison"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "VwTekigDghM0"
			},
			"source": [
				"### Dependencies and Environment Setup\n",
				"This section ensures the notebook is reproducible and fully operational across different environments, including Google Colab, AWS SageMaker, and local machines.<br/>\n",
				"Here it will install all necessary packages, set the working device (CPU/GPU), and configure paths and reproducibility settings."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:18.556631Z",
					"iopub.status.busy": "2025-07-16T13:19:18.556339Z",
					"iopub.status.idle": "2025-07-16T13:19:32.434856Z",
					"shell.execute_reply": "2025-07-16T13:19:32.434345Z",
					"shell.execute_reply.started": "2025-07-16T13:19:18.556601Z"
				},
				"id": "VopeGTnSghM1"
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"import subprocess\n",
				"\n",
				"def install(package):\n",
				"    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
				"\n",
				"install(\"ftfy\")\n",
				"install(\"regex\")\n",
				"install(\"tqdm\")\n",
				"install(\"scikit-learn\")\n",
				"install(\"scikit-image\")\n",
				"install(\"pooch\")\n",
				"install(\"matplotlib\")\n",
				"install(\"pillow\")\n",
				"install(\"openai-clip\")\n",
				"install(\"torch>=2.0\")\n",
				"install(\"torchvision\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:32.436359Z",
					"iopub.status.busy": "2025-07-16T13:19:32.435999Z",
					"iopub.status.idle": "2025-07-16T13:19:43.852322Z",
					"shell.execute_reply": "2025-07-16T13:19:43.851709Z",
					"shell.execute_reply.started": "2025-07-16T13:19:32.436327Z"
				},
				"id": "QtqdSOr8qqOn",
				"outputId": "188080f0-3d02-4399-d990-65997a1eeb6e"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using device: cuda\n"
					]
				}
			],
			"source": [
				"import os\n",
				"import random\n",
				"import numpy as np\n",
				"import matplotlib.pyplot as plt\n",
				"from PIL import Image\n",
				"from pathlib import Path\n",
				"from tqdm import tqdm\n",
				"\n",
				"import torch\n",
				"import torch.nn as nn\n",
				"from torch.utils.tensorboard import SummaryWriter\n",
				"import torchvision\n",
				"\n",
				"import clip  # OpenAI CLIP\n",
				"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
				"\n",
				"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
				"print(\"Using device:\", device)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.854141Z",
					"iopub.status.busy": "2025-07-16T13:19:43.853667Z",
					"iopub.status.idle": "2025-07-16T13:19:43.864937Z",
					"shell.execute_reply": "2025-07-16T13:19:43.862889Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.854119Z"
				},
				"id": "arDGJv6xghM2"
			},
			"outputs": [],
			"source": [
				"def set_seed(seed=42):\n",
				"    random.seed(seed)\n",
				"    np.random.seed(seed)\n",
				"    torch.manual_seed(seed)\n",
				"    torch.cuda.manual_seed_all(seed)\n",
				"\n",
				"    # Ensure deterministic behavior\n",
				"    torch.backends.cudnn.deterministic = True\n",
				"    torch.backends.cudnn.benchmark = False\n",
				"\n",
				"set_seed(42)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.865890Z",
					"iopub.status.busy": "2025-07-16T13:19:43.865689Z",
					"iopub.status.idle": "2025-07-16T13:19:43.869122Z",
					"shell.execute_reply": "2025-07-16T13:19:43.868637Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.865872Z"
				},
				"id": "M9V0U92FghM2"
			},
			"outputs": [],
			"source": [
				"# For saving checkpoints, plots, etc.\n",
				"Path(\"outputs\").mkdir(exist_ok=True)\n",
				"Path(\"checkpoints\").mkdir(exist_ok=True)\n",
				"Path(\"logs\").mkdir(exist_ok=True)\n",
				"Path(\"data\").mkdir(exist_ok=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "2353MHw1p24h"
			},
			"source": [
				"### Dataset Loading and Split\n",
				"Downloading the data directly from torchvision."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.872565Z",
					"iopub.status.busy": "2025-07-16T13:19:43.872361Z",
					"iopub.status.idle": "2025-07-16T13:19:43.876054Z",
					"shell.execute_reply": "2025-07-16T13:19:43.875515Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.872541Z"
				},
				"id": "M_1CrUhZpVCq"
			},
			"outputs": [],
			"source": [
				"def get_data(data_dir=\"./data\", transform=None):\n",
				"    \"\"\"Load Flowers102 train, validation and test sets.\n",
				"    Args:\n",
				"        data_dir (str): Directory where the dataset will be stored.\n",
				"        transform (torch.Compose)\n",
				"    Returns:\n",
				"        tuple: A tuple containing the train, validation, and test sets.\n",
				"    \"\"\"\n",
				"    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
				"    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
				"    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
				"    return train, val, test"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "KJI_a5EizA5a"
			},
			"source": [
				"#### Base and Novel categories\n",
				"The Oxford Flowers dataset contains 102 classes of flowers. For our experiments, we will split the dataset into base and novel categories. The first 51 classes will be used as base categories, while the remaining 51 classes will be treated as novel categories. This split allows us to simulate a real-world scenario where the model is trained on a set of known categories (base) and evaluated on unseen categories (novel)."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.878769Z",
					"iopub.status.busy": "2025-07-16T13:19:43.878599Z",
					"iopub.status.idle": "2025-07-16T13:19:43.882128Z",
					"shell.execute_reply": "2025-07-16T13:19:43.881580Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.878753Z"
				},
				"id": "nfq51vd8q_5a"
			},
			"outputs": [],
			"source": [
				"def base_novel_categories(dataset):\n",
				"    # set returns the unique set of all dataset classes\n",
				"    all_classes = set(dataset._labels)\n",
				"    # and let's count them\n",
				"    num_classes = len(all_classes)\n",
				"\n",
				"    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
				"    # then we slice the list in half and generate base and novel category lists\n",
				"    base_classes = list(range(num_classes))[:num_classes//2]\n",
				"    novel_classes = list(range(num_classes))[num_classes//2:]\n",
				"    return base_classes, novel_classes"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def base_prototype_embeddings(model, dataset, base_classes, device= \"cuda\"):\n",
				"    \"\"\"Compute the prototype embeddings for the base classes.\"\"\"\n",
				"    from collections import defaultdict\n",
				"    model.eval()\n",
				"\n",
				"    class_embeddings = defaultdict(list)\n",
				"\n",
				"    # Iterate through the dataset and collect embeddings for base classes\n",
				"    with torch.no_grad():\n",
				"        for i in tqdm(range(len(dataset)), desc=\"Computing base prototypes\"):\n",
				"            image, label = dataset[i]\n",
				"            if label in base_classes:\n",
				"                image = image.unsqueeze(0).to(device)\n",
				"                embedding = model.encode_image(image)  \n",
				"                class_embeddings[label].append(embedding.squeeze(0).cpu())  \n",
				"\n",
				"    # Compute the mean embedding for each base class\n",
				"    prototypes = []\n",
				"    for cls in base_classes:\n",
				"        if class_embeddings[cls]:\n",
				"            cls_tensor = torch.stack(class_embeddings[cls]) \n",
				"            prototype = cls_tensor.mean(dim=0)              \n",
				"        else:\n",
				"            raise ValueError(f\"No embeddings found for class {cls}\")\n",
				"        prototypes.append(prototype)\n",
				"\n",
				"    return torch.stack(prototypes).to(device)  \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {},
			"outputs": [],
			"source": [
				"# load descriptions from the JSON file\n",
				"# import json\n",
				"# def load_descriptions(file_path):\n",
				"#     with open(file_path, 'r') as f:\n",
				"#         data = json.load(f)\n",
				"#     descriptions = {}\n",
				"#     for item in data:\n",
				"#         class_name = item[\"class\"]\n",
				"#         description = item[\"description\"]\n",
				"#         descriptions[class_name] = {\"description\": description}\n",
				"    \n",
				"#     return descriptions\n",
				"\n",
				"# # Load descriptions\n",
				"# descriptions_file = \"descriptions.json\"\n",
				"# descriptions = load_descriptions(descriptions_file)\n",
				" "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "kvDdoYQr2fIu"
			},
			"source": [
				"#### Inspect Classes\n",
				"To inspect the classes, we will first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we will split it using `base_novel_categories`. Finally, we will use the hard-coded `CLASS_NAMES` to print the class names in natural language."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.885453Z",
					"iopub.status.busy": "2025-07-16T13:19:43.885184Z",
					"iopub.status.idle": "2025-07-16T13:19:43.960082Z",
					"shell.execute_reply": "2025-07-16T13:19:43.959575Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.885427Z"
				},
				"id": "veGGpNDctCgR",
				"outputId": "3a95f8a3-5878-41e1-d8d8-ba21390562ef"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Base Classes:\n",
						"  0: pink primrose\n",
						"  1: hard-leaved pocket orchid\n",
						"  2: canterbury bells\n",
						"  3: sweet pea\n",
						"  4: english marigold\n",
						"  5: tiger lily\n",
						"  6: moon orchid\n",
						"  7: bird of paradise\n",
						"  8: monkshood\n",
						"  9: globe thistle\n",
						" 10: snapdragon\n",
						" 11: colt's foot\n",
						" 12: king protea\n",
						" 13: spear thistle\n",
						" 14: yellow iris\n",
						" 15: globe-flower\n",
						" 16: purple coneflower\n",
						" 17: peruvian lily\n",
						" 18: balloon flower\n",
						" 19: giant white arum lily\n",
						" 20: fire lily\n",
						" 21: pincushion flower\n",
						" 22: fritillary\n",
						" 23: red ginger\n",
						" 24: grape hyacinth\n",
						" 25: corn poppy\n",
						" 26: prince of wales feathers\n",
						" 27: stemless gentian\n",
						" 28: artichoke\n",
						" 29: sweet william\n",
						" 30: carnation\n",
						" 31: garden phlox\n",
						" 32: love in the mist\n",
						" 33: mexican aster\n",
						" 34: alpine sea holly\n",
						" 35: ruby-lipped cattleya\n",
						" 36: cape flower\n",
						" 37: great masterwort\n",
						" 38: siam tulip\n",
						" 39: lenten rose\n",
						" 40: barbeton daisy\n",
						" 41: daffodil\n",
						" 42: sword lily\n",
						" 43: poinsettia\n",
						" 44: bolero deep blue\n",
						" 45: wallflower\n",
						" 46: marigold\n",
						" 47: buttercup\n",
						" 48: oxeye daisy\n",
						" 49: common dandelion\n",
						" 50: petunia\n",
						"\n",
						"Novel Classes:\n",
						" 51: wild pansy\n",
						" 52: primula\n",
						" 53: sunflower\n",
						" 54: pelargonium\n",
						" 55: bishop of llandaff\n",
						" 56: gaura\n",
						" 57: geranium\n",
						" 58: orange dahlia\n",
						" 59: pink-yellow dahlia?\n",
						" 60: cautleya spicata\n",
						" 61: japanese anemone\n",
						" 62: black-eyed susan\n",
						" 63: silverbush\n",
						" 64: californian poppy\n",
						" 65: osteospermum\n",
						" 66: spring crocus\n",
						" 67: bearded iris\n",
						" 68: windflower\n",
						" 69: tree poppy\n",
						" 70: gazania\n",
						" 71: azalea\n",
						" 72: water lily\n",
						" 73: rose\n",
						" 74: thorn apple\n",
						" 75: morning glory\n",
						" 76: passion flower\n",
						" 77: lotus\n",
						" 78: toad lily\n",
						" 79: anthurium\n",
						" 80: frangipani\n",
						" 81: clematis\n",
						" 82: hibiscus\n",
						" 83: columbine\n",
						" 84: desert-rose\n",
						" 85: tree mallow\n",
						" 86: magnolia\n",
						" 87: cyclamen\n",
						" 88: watercress\n",
						" 89: canna lily\n",
						" 90: hippeastrum\n",
						" 91: bee balm\n",
						" 92: ball moss\n",
						" 93: foxglove\n",
						" 94: bougainvillea\n",
						" 95: camellia\n",
						" 96: mallow\n",
						" 97: mexican petunia\n",
						" 98: bromelia\n",
						" 99: blanket flower\n",
						"100: trumpet creeper\n",
						"101: blackberry lily\n",
						"\n"
					]
				}
			],
			"source": [
				"_, _, tmp_test = get_data()\n",
				"base_classes, novel_classes = base_novel_categories(tmp_test)\n",
				"CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
				"\n",
				"# Pretty formatted print of base and novel classes\n",
				"def print_classes(label, classes, class_names):\n",
				"    print(f\"{label} Classes:\")\n",
				"    for i in classes:\n",
				"        print(f\"{i:3d}: {class_names[i]}\")\n",
				"    print()\n",
				"\n",
				"print_classes(\"Base\", base_classes, CLASS_NAMES)\n",
				"print_classes(\"Novel\", novel_classes, CLASS_NAMES)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "8puO1VNpzwvi"
			},
			"source": [
				"#### Split Dataset\n",
				"The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
				"To split the data we need the dataset and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.963212Z",
					"iopub.status.busy": "2025-07-16T13:19:43.962918Z",
					"iopub.status.idle": "2025-07-16T13:19:43.967103Z",
					"shell.execute_reply": "2025-07-16T13:19:43.966574Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.963185Z"
				},
				"id": "msOszMs2zRRu"
			},
			"outputs": [],
			"source": [
				"def split_data(dataset, base_classes):\n",
				"    # these two lists will store the sample indexes\n",
				"    base_categories_samples = []\n",
				"    novel_categories_samples = []\n",
				"\n",
				"    # we create a set of base classes to compute the test below in O(1)\n",
				"    # this is optional and can be removed\n",
				"    base_set = set(base_classes)\n",
				"\n",
				"    # here we iterate over sample labels and also get the correspondent sample index\n",
				"    for sample_id, label in enumerate(dataset._labels):\n",
				"        if label in base_set:\n",
				"            base_categories_samples.append(sample_id)\n",
				"        else:\n",
				"            novel_categories_samples.append(sample_id)\n",
				"\n",
				"    # here we create the dataset subsets\n",
				"    # the torch Subset is just a wrapper around the dataset\n",
				"    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
				"    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
				"    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
				"    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
				"    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
				"    return base_dataset, novel_dataset"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "VQZT22rE8hBw"
			},
			"source": [
				"#### Extract k shots\n",
				"As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
				"Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "-KpbPRLr7WL_"
			},
			"source": [
				"### CLIP (ViT-B/16) Loading and Inspection"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:43.972385Z",
					"iopub.status.busy": "2025-07-16T13:19:43.972211Z",
					"iopub.status.idle": "2025-07-16T13:19:49.700103Z",
					"shell.execute_reply": "2025-07-16T13:19:49.699496Z",
					"shell.execute_reply.started": "2025-07-16T13:19:43.972369Z"
				},
				"id": "Sh6uLZRT7YJx",
				"outputId": "e695fe2f-32b0-419e-b8c4-de5a8eadcacf"
			},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"Compose(\n",
							"    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
							"    CenterCrop(size=(224, 224))\n",
							"    <function _convert_image_to_rgb at 0x000002546289CC20>\n",
							"    ToTensor()\n",
							"    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
							")"
						]
					},
					"execution_count": 11,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
				"model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
				"\n",
				"# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
				"preprocess\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "lM9H14899ses"
			},
			"source": [
				"#### Load and Prepare Data\n",
				"Here we get the three dataset split and pass clip pre-defined augmentations.\n",
				"Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
				"Finally, we split the three datasets into base and novel categories.\n",
				"As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:49.702970Z",
					"iopub.status.busy": "2025-07-16T13:19:49.702779Z",
					"iopub.status.idle": "2025-07-16T13:19:49.729195Z",
					"shell.execute_reply": "2025-07-16T13:19:49.728714Z",
					"shell.execute_reply.started": "2025-07-16T13:19:49.702952Z"
				},
				"id": "TVrYUYTv9ttM"
			},
			"outputs": [],
			"source": [
				"# get the three datasets\n",
				"train_set, val_set, test_set = get_data(transform=preprocess)\n",
				"\n",
				"# split classes into base and novel\n",
				"base_classes, novel_classes = base_novel_categories(train_set)\n",
				"\n",
				"# split the three datasets\n",
				"train_base, _ = split_data(train_set, base_classes)\n",
				"val_base, _ = split_data(val_set, base_classes)\n",
				"test_base, test_novel = split_data(test_set, base_classes)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "YcgMwr3J9VIg"
			},
			"source": [
				"### Compute Zero-Shot Predictions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/",
					"height": 526
				},
				"execution": {
					"iopub.execute_input": "2025-07-16T13:19:49.732288Z",
					"iopub.status.busy": "2025-07-16T13:19:49.732069Z",
					"iopub.status.idle": "2025-07-16T13:19:49.737660Z",
					"shell.execute_reply": "2025-07-16T13:19:49.737139Z",
					"shell.execute_reply.started": "2025-07-16T13:19:49.732264Z"
				},
				"id": "7uhblkvm9US4",
				"outputId": "dd9a232f-37a7-4229-e133-6a27661cda3d"
			},
			"outputs": [],
			"source": [
				"@torch.no_grad() # we don't want gradients\n",
				"def eval(model, dataset, categories, batch_size, device, label=\"\"):\n",
				"    # let's set the model in evaluation mode\n",
				"    model.eval()\n",
				"\n",
				"    # Remap labels into a contiguous set starting from zero\n",
				"    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
				"\n",
				"    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
				"    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
				"    text_inputs = clip.tokenize(\n",
				"        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
				"    ).to(device)\n",
				"\n",
				"    # we can encode the text features once as they are shared for all images\n",
				"    # therefore we do it outside the evaluation loop\n",
				"    text_features = model.encode_text(text_inputs)\n",
				"    # and here we normalize them (standard pratice with CLIP)\n",
				"    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
				"\n",
				"    # simple dataloader creation\n",
				"    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
				"\n",
				"    # here we store the number of correct predictions we will make\n",
				"    correct_predictions = 0\n",
				"    for image, target in tqdm(dataloader, desc=label):\n",
				"        # base categories range from 0 to 50, while novel ones from 51 to 101\n",
				"        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
				"        # Map targets in contiguous set starting from zero\n",
				"        # Labels needs to be .long() in pytorch\n",
				"        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
				"\n",
				"        image = image.to(device)\n",
				"        target = target.to(device)\n",
				"\n",
				"        # forward image through CLIP image encoder\n",
				"        image_features = model.encode_image(image)\n",
				"        # and normalize\n",
				"        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
				"\n",
				"        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
				"        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
				"        # now we check which are correct, and sum them (False == 0, True == 1)\n",
				"        correct_predictions += (predicted_class == target).sum().item()\n",
				"\n",
				"    # and now we compute the accuracy\n",
				"    accuracy = correct_predictions / len(dataset)\n",
				"    return accuracy"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
				"novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
				"\n",
				"print()\n",
				"print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
				"print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "baYfLKNdfbUR"
			},
			"source": [
				"#### Harmonic Mean\n",
				"Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
				"The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
				"Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:19.976878Z",
					"iopub.status.busy": "2025-07-16T13:20:19.976547Z",
					"iopub.status.idle": "2025-07-16T13:20:19.980559Z",
					"shell.execute_reply": "2025-07-16T13:20:19.980098Z",
					"shell.execute_reply.started": "2025-07-16T13:20:19.976842Z"
				},
				"id": "rKAXR7hlfbUR"
			},
			"outputs": [],
			"source": [
				"def harmonic_mean(base_accuracy, novel_accuracy):\n",
				"    numerator = 2\n",
				"    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
				"    hm = numerator / denominator\n",
				"    return hm\n",
				"\n",
				"# print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "nTfBAJovghM6"
			},
			"source": [
				"### Baseline evaluation and Plots"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "trHBv5NEghM6"
			},
			"source": [
				"## Methodology: CoCoOp\n",
				"##### TODO\n",
				"- Provide a short literature summary of CoCoOp with diagram (can be ASCII or markdown)\n",
				"    - What it is (image-conditioned prompt learning)\n",
				"    - Why it helps (avoids overfitting class tokens, learns better generalization)\n",
				"- Describe the architecture:\n",
				"    - MetaNet / PromptLearner: learn image-conditioned soft prompts\n",
				"    - Prompt format: [CLS] + ctx_tokens + class name + EOS\n",
				"    - Only prompt tokens and MetaNet are updated, CLIP remains frozen\n",
				"- Present equations:\n",
				"    - Prompt generation\n",
				"    - Similarity computation\n",
				"    - Cross-entropy loss over cosine similarity\n",
				"- Show a schematic of the training pipeline (optional visual diagram)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "mwV3pf7YghM6"
			},
			"source": [
				"## Implementation: CoCoOp Baseline\n",
				"In this section, we implement the CoCoOp (Conditional Context Optimization) method for adapting CLIP to our downstream task. CoCoOp extends the original CoOp approach by generating prompts that are conditioned on the input image. The only trainable components are:\n",
				"\n",
				"- A PromptLearner module that generates learnable context tokens\n",
				"- A small MetaNet (image-conditional MLP) that produces dynamic prompt embeddings\n",
				"\n",
				"We freeze CLIP's visual and text encoders and only train the prompt learner to minimize cross-entropy over the Base training set.\n",
				"\n",
				"##### TODO\n",
				"- Create PromptLearner module (copy from lab.py and adapt for Oxford Flowers)\n",
				"- Load CLIP\n",
				"- Freeze CLIP weights\n",
				"- Setup train/val/test splits from base classes only\n",
				"- Train PromptLearner on base classes\n",
				"- Use tokenizer and pre-tokenized text prompts for classnames\n",
				"- Implement optimizer, scheduler, and loss function\n",
				"- Report model summary (trainable params)\n",
				"- Show training logs: loss and accuracy curves"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:19.987492Z",
					"iopub.status.busy": "2025-07-16T13:20:19.987110Z",
					"iopub.status.idle": "2025-07-16T13:20:20.081394Z",
					"shell.execute_reply": "2025-07-16T13:20:20.080898Z",
					"shell.execute_reply.started": "2025-07-16T13:20:19.987464Z"
				},
				"id": "ThDIpaP7ghM6"
			},
			"outputs": [],
			"source": [
				"import torch\n",
				"import torch.nn as nn\n",
				"import torch.nn.functional as F\n",
				"from collections import OrderedDict\n",
				"from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
				"from torch.utils.data import DataLoader\n",
				"\n",
				"\n",
				"_tokenizer = _Tokenizer()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:20.082271Z",
					"iopub.status.busy": "2025-07-16T13:20:20.082067Z",
					"iopub.status.idle": "2025-07-16T13:20:20.087554Z",
					"shell.execute_reply": "2025-07-16T13:20:20.087043Z",
					"shell.execute_reply.started": "2025-07-16T13:20:20.082253Z"
				},
				"id": "pCo-rRO2ghM6"
			},
			"outputs": [],
			"source": [
				"class TextEncoder(nn.Module):\n",
				"    def __init__(self, clip_model):\n",
				"        super().__init__()\n",
				"        self.transformer = clip_model.transformer\n",
				"        self.positional_embedding = clip_model.positional_embedding\n",
				"        self.ln_final = clip_model.ln_final\n",
				"        self.text_projection = clip_model.text_projection\n",
				"        self.dtype = clip_model.dtype\n",
				"\n",
				"    def forward(self, prompts, tokenized_prompts):\n",
				"        x = prompts.to(self.dtype) + self.positional_embedding.to(self.dtype)\n",
				"        x = x.permute(1, 0, 2)  # NLD -> LND\n",
				"        x = self.transformer(x)\n",
				"        x = x.permute(1, 0, 2)  # LND -> NLD\n",
				"        x = self.ln_final(x).type(self.dtype)\n",
				"\n",
				"        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
				"        return x @ self.text_projection"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:20.088549Z",
					"iopub.status.busy": "2025-07-16T13:20:20.088173Z",
					"iopub.status.idle": "2025-07-16T13:20:20.097301Z",
					"shell.execute_reply": "2025-07-16T13:20:20.096778Z",
					"shell.execute_reply.started": "2025-07-16T13:20:20.088531Z"
				},
				"id": "7f2JfmGbghM7"
			},
			"outputs": [],
			"source": [
				"class PromptLearner(nn.Module):\n",
				"    def __init__(self, class_ids, class_names, clip_model, n_ctx=4, ctx_init=None,  prototype_embeddings=None,device=\"cuda\"):\n",
				"        super().__init__()\n",
				"        self.dtype = clip_model.dtype\n",
				"        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
				"        vis_dim = clip_model.visual.output_dim\n",
				"        self.device = device\n",
				"        self.n_cls = len(class_ids)\n",
				"        self.n_ctx = n_ctx\n",
				"        self.prototype_embeddings = prototype_embeddings  \n",
				"        # self.descriptions = descriptions  # dict with descriptions\n",
				"\n",
				"        self.class_ids = class_ids\n",
				"        self.classnames = [class_names[c].replace(\"_\", \" \") for c in class_ids]\n",
				"\n",
				"        # desc_list = []\n",
				"        # for name in self.classnames[:50]:\n",
				"        #     desc = self.descriptions[name][\"description\"]\n",
				"        #     desc = \" \".join(desc.split())[:77]  # token length approx.\n",
				"        #     desc_list.append(desc)\n",
				"        # descriptions_tokenized = clip.tokenize(desc_list).to(device)\n",
				"        # with torch.no_grad():\n",
				"        #     desc_embeddings_full = clip_model.token_embedding(descriptions_tokenized).to(self.dtype)\n",
				"        #     self.desc_embeddings = desc_embeddings_full.mean(dim=1)\n",
				"\n",
				"        if ctx_init:\n",
				"            ctx_init = ctx_init.replace(\"_\", \" \")\n",
				"            n_ctx = len(ctx_init.split(\" \"))\n",
				"            # prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
				"            prompt = clip.tokenize(ctx_init).to(device)\n",
				"\n",
				"            # descriptions_tokenized = clip.tokenize(desc_list).to(device)\n",
				"            with torch.no_grad():\n",
				"                embedding = clip_model.token_embedding(prompt).to(self.dtype)\n",
				"                \n",
				"            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
				"            prompt_prefix = ctx_init\n",
				"        else:\n",
				"            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
				"            nn.init.normal_(ctx_vectors, std=0.02)\n",
				"            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
				"\n",
				"        print(f'Initial context: \"{prompt_prefix}\"')\n",
				"        print(f\"# of context tokens: {n_ctx}\")\n",
				"\n",
				"        # desc_sample = self.desc_embeddings[0]  # tensor shape (tokens, dim)\n",
				"        # desc_dim = desc_sample.shape[-1]      # typically 512\n",
				"\n",
				"        self.ctx = nn.Parameter(ctx_vectors)\n",
				"\n",
				"        self.meta_net = nn.Sequential(OrderedDict([\n",
				"            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
				"            (\"relu\", nn.ReLU(inplace=True)),\n",
				"            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
				"        ])).to(device)\n",
				"\n",
				"        self.meta_net_prototype = nn.Sequential(OrderedDict([\n",
				"            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
				"            (\"relu\", nn.ReLU(inplace=True)),\n",
				"            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
				"        ]))\n",
				"\n",
				"        # self.meta_net_desc = nn.Sequential(OrderedDict([\n",
				"        #     (\"linear1\", nn.Linear(desc_dim, desc_dim // 16)),\n",
				"        #     (\"relu\", nn.ReLU(inplace=True)),\n",
				"        #     (\"linear2\", nn.Linear(desc_dim // 16, ctx_dim))\n",
				"        # ]))\n",
				"\n",
				"        self.fusion_gate = nn.Sequential(\n",
				"            nn.Linear(2 * ctx_dim, ctx_dim),\n",
				"            nn.Sigmoid()\n",
				"        )\n",
				"\n",
				"        prompts = [prompt_prefix + \" \" + name + \".\" for name in self.classnames]\n",
				"        # tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
				"        tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
				"        with torch.no_grad():\n",
				"            embedding = clip_model.token_embedding(tokenized).to(self.dtype)\n",
				"\n",
				"        self.register_buffer(\"token_prefix\", embedding[:, :1, :]) #SOS\n",
				"        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :]) #CLS, EOS\n",
				"        self.tokenized_prompts = tokenized\n",
				"\n",
				"    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
				"        if label is not None:\n",
				"            prefix = prefix[label]\n",
				"            suffix = suffix[label]\n",
				"        return torch.cat([prefix, ctx, suffix], dim=1)\n",
				"\n",
				"    def forward(self, image_features,label):\n",
				"        \"\"\" Forward pass to compute the context-shifted prompts with the additional bias. \"\"\"\n",
				"        B = image_features.shape[0]\n",
				"        ctx = self.ctx.to(dtype=torch.float32, device=self.device).unsqueeze(0)  # (1, n_ctx, dim)\n",
				"\n",
				"        # Original image bias\n",
				"        bias_img = self.meta_net(image_features.to(dtype=torch.float32, device=self.device)).to(self.dtype).unsqueeze(1)  # (B, 1, dim)\n",
				"\n",
				"        # Class prototypes\n",
				"        class_proto = self.prototype_embeddings[label].to(dtype=torch.float32, device=self.device)  # (B, vis_dim)\n",
				"        bias_proto = self.meta_net_prototype(class_proto).to(dtype=self.dtype, device=self.device).unsqueeze(1)  # (B, 1, dim)\n",
				"\n",
				"        # Descriptions\n",
				"        # class_desc = self.desc_embeddings[labels]  # (B, desc_dim)\n",
				"        # bias_desc = self.meta_net_desc(class_desc)  # (B, ctx_dim)\n",
				"        \n",
				"        # Evaluate gate\n",
				"        concat = torch.cat([bias_img, bias_proto], dim=-1).squeeze(1)\n",
				"        gate = self.fusion_gate(concat.to(dtype=torch.float32)).unsqueeze(1)  # (B, 1, ctx_dim)\n",
				"\n",
				"        # Evalute total bias\n",
				"        total_bias = gate * bias_img + (1 - gate) * bias_proto  # (B, 1, ctx_dim)\n",
				"\n",
				"        ctx_shifted = ctx + total_bias  # (B, n_ctx, dim)\n",
				"\n",
				"        prefix = self.token_prefix\n",
				"        suffix = self.token_suffix\n",
				"\n",
				"        prompts = []\n",
				"        for ctx_i in ctx_shifted:\n",
				"            ctx_exp = ctx_i.unsqueeze(0).expand(self.n_cls, -1, -1)       # (n_cls, n_ctx, dim)\n",
				"            pts = self.construct_prompts(ctx_exp, prefix, suffix)         # (n_cls, ?, dim)\n",
				"            prompts.append(pts)\n",
				"        return torch.stack(prompts)  # (B, n_cls, ?, dim)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:20.098205Z",
					"iopub.status.busy": "2025-07-16T13:20:20.097899Z",
					"iopub.status.idle": "2025-07-16T13:20:20.104884Z",
					"shell.execute_reply": "2025-07-16T13:20:20.104403Z",
					"shell.execute_reply.started": "2025-07-16T13:20:20.098187Z"
				},
				"id": "RCAoQDpgghM7"
			},
			"outputs": [],
			"source": [
				"class CoCoOp(nn.Module):\n",
				"    def __init__(self, class_ids, class_names, clip_model, n_ctx=4, ctx_init=None, device=\"cuda\"):\n",
				"        super().__init__()\n",
				"        self.device = device\n",
				"        prototype_embeddings = base_prototype_embeddings(clip_model, train_base, class_ids, device=device)\n",
				"        self.prompt_learner = PromptLearner(class_ids, class_names, clip_model, n_ctx=n_ctx, ctx_init=ctx_init, device=device,prototype_embeddings=prototype_embeddings).to(device)\n",
				"        self.clip_image_encoder = clip_model.visual\n",
				"        self.clip_text_encoder = clip_model.encode_text\n",
				"        self.text_encoder = TextEncoder(clip_model).to(device)\n",
				"        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
				"        self.logit_scale = clip_model.logit_scale\n",
				"        self.dtype = clip_model.dtype\n",
				"        self.class_ids = class_ids\n",
				"\n",
				"        # Freeze everything except prompt learner\n",
				"        for name, param in self.named_parameters():\n",
				"            if \"prompt_learner\" not in name:\n",
				"                param.requires_grad_(False)\n",
				"\n",
				"        print(f\"Total parameters: {sum(p.numel() for p in self.parameters())}\")\n",
				"        print(f\"Trainable parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\")\n",
				"        print(f\"Trainable parameter percentage: {sum(p.numel() for p in self.parameters() if p.requires_grad) / sum(p.numel() for p in self.parameters()) * 100:.2f}%\")\n",
				"\n",
				"    # For compatibility with the original CLIP interface\n",
				"    def encode_text(self, text_inputs):\n",
				"        return self.clip_text_encoder(text_inputs)\n",
				"\n",
				"    # For compatibility with the original CLIP interface\n",
				"    def encode_image(self, image):\n",
				"        return self.clip_image_encoder(image.to(dtype=self.dtype))\n",
				"\n",
				"    def forward(self, images, labels):\n",
				"        images = images.to(self.device).to(self.dtype)\n",
				"        image_features = self.clip_image_encoder(images)\n",
				"        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
				"        prompts = self.prompt_learner(image_features, labels)  # (B, n_cls, ?, dim)\n",
				"\n",
				"        logits = []\n",
				"        for p_i, i_f in zip(prompts, image_features):\n",
				"            text_features = self.text_encoder(p_i, self.tokenized_prompts)\n",
				"            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
				"            logit = self.logit_scale.exp() * i_f @ text_features.T\n",
				"            logits.append(logit)\n",
				"        logits = torch.stack(logits)\n",
				"\n",
				"        # if self.training and labels is not None:\n",
				"        #     return F.cross_entropy(logits, labels)\n",
				"\n",
				"        return logits"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "vdwqkdDYghM7"
			},
			"source": [
				"We now define the training and evaluation logic for CoCoOp using the Base training classes only. The model is trained to optimize cross-entropy loss over class predictions."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 46,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:20.110193Z",
					"iopub.status.busy": "2025-07-16T13:20:20.109860Z",
					"iopub.status.idle": "2025-07-16T13:20:20.119905Z",
					"shell.execute_reply": "2025-07-16T13:20:20.119404Z",
					"shell.execute_reply.started": "2025-07-16T13:20:20.110175Z"
				},
				"id": "WrbeKo-9wJYJ"
			},
			"outputs": [],
			"source": [
				"def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\", desc=\"Training\"):\n",
				"  samples = 0.0\n",
				"  cumulative_loss = 0.0\n",
				"  cumulative_accuracy = 0.0\n",
				"\n",
				"  net.train()\n",
				"\n",
				"  pbar = tqdm(data_loader, desc=desc, position=0, leave=True, total=len(data_loader))\n",
				"  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
				"    inputs = inputs.to(device)\n",
				"    targets = targets.to(device)\n",
				"\n",
				"    optimizer.zero_grad()\n",
				"    outputs = net(inputs,targets)\n",
				"    loss = cost_function(outputs, targets)\n",
				"    loss.backward()\n",
				"    optimizer.step()\n",
				"\n",
				"    samples += inputs.shape[0]\n",
				"    cumulative_loss += loss.item()\n",
				"    _, predicted = outputs.max(dim=1)\n",
				"\n",
				"    cumulative_accuracy += (predicted == targets).sum().item()\n",
				"\n",
				"    pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
				"    pbar.update(1)\n",
				"    torch.cuda.empty_cache()\n",
				"\n",
				"  return cumulative_loss / samples, cumulative_accuracy /samples * 100\n",
				"\n",
				"def test_step(net, data_loader, cost_function, device=\"cuda\", desc=\"Testing\", categories=None):\n",
				"  samples = 0.0\n",
				"  cumulative_loss = 0.0\n",
				"  cumulative_accuracy = 0.0\n",
				"\n",
				"  net.eval()\n",
				"\n",
				"  # If categories are provided, build mapping for contiguous labels\n",
				"  if categories is not None:\n",
				"    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
				"\n",
				"  pbar = tqdm(data_loader, desc=desc, position=0, leave=True, total=len(data_loader))\n",
				"  with torch.no_grad():\n",
				"    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
				"      inputs = inputs.to(device)\n",
				"      targets = targets.to(device)\n",
				"\n",
				"      # Remap targets if categories are provided\n",
				"      if categories is not None:\n",
				"        targets = torch.Tensor([contig_cat2idx[t.item()] for t in targets]).long().to(device)\n",
				"\n",
				"      outputs = net(inputs,targets)\n",
				"      loss = cost_function(outputs, targets)\n",
				"      samples += inputs.shape[0]\n",
				"      cumulative_loss += loss.item()\n",
				"      _, predicted = outputs.max(dim=1)\n",
				"\n",
				"      cumulative_accuracy += (predicted == targets).sum().item()\n",
				"\n",
				"      pbar.set_postfix(test_loss=loss.item(), test_acc=cumulative_accuracy / samples * 100)\n",
				"      pbar.update(1)\n",
				"\n",
				"  return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
				"\n",
				"\n",
				"# A function similar to the test step, but for the novel classes, using a fixed prompt instead of learned prompts\n",
				"def test_novel_step(net, data_loader, cost_function, device=\"cuda\", desc=\"Testing Novel Classes\", categories=None):\n",
				"  samples = 0.0\n",
				"  cumulative_loss = 0.0\n",
				"  cumulative_accuracy = 0.0\n",
				"\n",
				"  net.eval()\n",
				"\n",
				"  # If categories are provided, build mapping for contiguous labels\n",
				"  if categories is not None:\n",
				"    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
				"\n",
				"  # Create a fixed prompt for each novel class\n",
				"  fixed_prompts = clip.tokenize(\n",
				"      [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
				"  ).to(device)\n",
				"  text_features = net.encode_text(fixed_prompts)\n",
				"  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
				"\n",
				"  pbar = tqdm(data_loader, desc=desc, position=0, leave=True, total=len(data_loader))\n",
				"  with torch.no_grad():\n",
				"    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
				"      inputs = inputs.to(device)\n",
				"      targets = targets.to(device)\n",
				"\n",
				"      # Remap targets if categories are provided\n",
				"      if categories is not None:\n",
				"        targets = torch.Tensor([contig_cat2idx[t.item()] for t in targets]).long().to(device)\n",
				"\n",
				"      image_features = net.encode_image(inputs)\n",
				"      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
				"      logits = image_features @ text_features.T\n",
				"\n",
				"      loss = cost_function(logits, targets)\n",
				"      samples += inputs.shape[0]\n",
				"      cumulative_loss += loss.item()\n",
				"      _, predicted = logits.max(dim=1)\n",
				"\n",
				"      cumulative_accuracy += (predicted == targets).sum().item()\n",
				"\n",
				"      pbar.set_postfix(test_loss=loss.item(), test_acc=cumulative_accuracy / samples * 100)\n",
				"      pbar.update(1)\n",
				"\n",
				"  return cumulative_loss / samples, cumulative_accuracy / samples * 100"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 47,
			"metadata": {
				"execution": {
					"iopub.execute_input": "2025-07-16T13:20:20.120726Z",
					"iopub.status.busy": "2025-07-16T13:20:20.120478Z",
					"iopub.status.idle": "2025-07-16T13:20:20.125251Z",
					"shell.execute_reply": "2025-07-16T13:20:20.124755Z",
					"shell.execute_reply.started": "2025-07-16T13:20:20.120708Z"
				},
				"id": "HM-I2NzmwJXA"
			},
			"outputs": [],
			"source": [
				"def get_optimizer(model, lr, wd, momentum):\n",
				"  optimizer = torch.optim.SGD(\n",
				"      [{\"params\": model.parameters()}],\n",
				"      lr=lr,\n",
				"      momentum=momentum,\n",
				"      weight_decay=wd,\n",
				"      nesterov=True,\n",
				"  )\n",
				"  return optimizer\n",
				"\n",
				"def get_cost_function():\n",
				"  cost_function = torch.nn.CrossEntropyLoss()\n",
				"  return cost_function"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 48,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"The tensorboard extension is already loaded. To reload it, use:\n",
						"  %reload_ext tensorboard\n"
					]
				},
				{
					"data": {
						"text/plain": [
							"Reusing TensorBoard on port 6006 (pid 5744), started 6 days, 3:46:09 ago. (Use '!kill 5744' to kill it.)"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"text/html": [
							"\n",
							"      <iframe id=\"tensorboard-frame-1a3d1fa7bc8960a9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
							"      </iframe>\n",
							"      <script>\n",
							"        (function() {\n",
							"          const frame = document.getElementById(\"tensorboard-frame-1a3d1fa7bc8960a9\");\n",
							"          const url = new URL(\"http://localhost\");\n",
							"          const port = 6006;\n",
							"          if (port) {\n",
							"            url.port = port;\n",
							"          }\n",
							"          frame.src = url;\n",
							"        })();\n",
							"      </script>\n",
							"    "
						],
						"text/plain": [
							"<IPython.core.display.HTML object>"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				}
			],
			"source": [
				"%load_ext tensorboard\n",
				"%tensorboard --logdir=runs"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 49,
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/",
					"height": 405
				},
				"execution": {
					"iopub.execute_input": "2025-07-16T13:48:09.768244Z",
					"iopub.status.busy": "2025-07-16T13:48:09.767914Z",
					"iopub.status.idle": "2025-07-16T14:10:21.697598Z",
					"shell.execute_reply": "2025-07-16T14:10:21.697048Z",
					"shell.execute_reply.started": "2025-07-16T13:48:09.768218Z"
				},
				"id": "9wF9GErBghM7",
				"outputId": "d360277b-c66d-4aa8-aa4c-28fbcffb89ed"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Training on 51 classes\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Computing base prototypes: 100%|██████████| 510/510 [00:15<00:00, 32.56it/s]\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Initial context: \"X X X X\"\n",
						"# of context tokens: 4\n",
						"Total parameters: 124917313\n",
						"Trainable parameters: 593472\n",
						"Trainable parameter percentage: 0.48%\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"Train Epoch 1/15:   0%|          | 0/510 [00:00<?, ?it/s]"
					]
				},
				{
					"ename": "IndexError",
					"evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
					"output_type": "error",
					"traceback": [
						"\u001b[31m---------------------------------------------------------------------------\u001b[39m",
						"\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
						"\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    165\u001b[39m     writer.add_figure(\u001b[33m\"\u001b[39m\u001b[33mCurves\u001b[39m\u001b[33m\"\u001b[39m, plt.gcf())\n\u001b[32m    166\u001b[39m     writer.close()\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43mmain_cocoop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
						"\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mmain_cocoop\u001b[39m\u001b[34m(batch_size, device, learning_rate, weight_decay, momentum, epochs, run_name, n_ctx, ctx_init)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         train_loss, train_acc = \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrain Epoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43me\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m         val_loss, val_acc = test_step(model, val_loader, cost_function, device=device, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m         train_losses.append(train_loss)\n",
						"\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtraining_step\u001b[39m\u001b[34m(net, data_loader, optimizer, cost_function, device, desc)\u001b[39m\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m     14\u001b[39m outputs = net(inputs,targets)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m loss = \u001b[43mcost_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss.backward()\n\u001b[32m     17\u001b[39m optimizer.step()\n",
						"\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jacot\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
						"\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jacot\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
						"\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jacot\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
						"\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jacot\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
						"\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
					]
				}
			],
			"source": [
				"def main_cocoop(\n",
				"    batch_size=1,\n",
				"    device=\"cuda:0\",\n",
				"    learning_rate=0.002,\n",
				"    weight_decay=0.0005,\n",
				"    momentum=0.9,\n",
				"    epochs=15,\n",
				"    run_name=\"CoCoOp_Proto2\",\n",
				"    n_ctx=4,\n",
				"    ctx_init=None,\n",
				"):\n",
				"    writer = SummaryWriter(f\"runs/{run_name}\")\n",
				"    \n",
				"    clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
				"    train_set, val_set, test_set = get_data(transform=preprocess)\n",
				"    base_classes, novel_classes = base_novel_categories(train_set)\n",
				"    train_base, _ = split_data(train_set, base_classes)\n",
				"    val_base, _ = split_data(val_set, base_classes)\n",
				"    test_base, test_novel = split_data(test_set, base_classes)\n",
				"\n",
				"    print(f\"Training on {len(base_classes)} classes\")\n",
				"\n",
				"    train_loader = DataLoader(train_base, batch_size=batch_size, shuffle=True, num_workers=2)\n",
				"    val_loader = DataLoader(val_base, batch_size=batch_size, shuffle=False, num_workers=2)\n",
				"    test_loader_base = DataLoader(test_base, batch_size=batch_size, shuffle=False, num_workers=2)\n",
				"    test_loader_novel = DataLoader(test_novel, batch_size=batch_size, shuffle=False, num_workers=2)\n",
				"\n",
				"    model = CoCoOp(base_classes, CLASS_NAMES, clip_model, n_ctx=n_ctx, ctx_init=ctx_init, device=device)\n",
				"    optimizer = get_optimizer(model, learning_rate, weight_decay, momentum)\n",
				"    cost_function = get_cost_function()\n",
				"\n",
				"    train_losses, val_losses = [], []\n",
				"    train_accs, val_accs = [], []\n",
				"\n",
				"    # Log hyperparameters\n",
				"    writer.add_hparams(\n",
				"        {\n",
				"            \"batch_size\": batch_size,\n",
				"            \"learning_rate\": learning_rate,\n",
				"            \"weight_decay\": weight_decay,\n",
				"            \"momentum\": momentum,\n",
				"            \"epochs\": epochs,\n",
				"            \"n_ctx\": n_ctx,\n",
				"        }, {}\n",
				"    )\n",
				"\n",
				"    best_val_acc = 0.0\n",
				"    best_epoch = -1\n",
				"    checkpoint_path = f\"checkpoints/{run_name}_best.pt\"\n",
				"    last_checkpoint_path = f\"checkpoints/{run_name}_last.pt\"\n",
				"    early_stop_patience = 5\n",
				"    patience_counter = 0\n",
				"\n",
				"    try:\n",
				"        for e in range(epochs):\n",
				"            train_loss, train_acc = training_step(model, train_loader, optimizer, cost_function, device=device, desc=f\"Train Epoch {e+1}/{epochs}\")\n",
				"            val_loss, val_acc = test_step(model, val_loader, cost_function, device=device, desc=f\"Valid Epoch {e+1}/{epochs}\")\n",
				"\n",
				"            train_losses.append(train_loss)\n",
				"            val_losses.append(val_loss)\n",
				"            train_accs.append(train_acc)\n",
				"            val_accs.append(val_acc)\n",
				"\n",
				"            writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Val\": val_loss}, e)\n",
				"            writer.add_scalars(\"Accuracy\", {\"Train\": train_acc, \"Val\": val_acc}, e)\n",
				"            writer.flush()\n",
				"\n",
				"            # Checkpoint: Save best model\n",
				"            if val_acc > best_val_acc:\n",
				"                best_val_acc = val_acc\n",
				"                best_epoch = e\n",
				"                patience_counter = 0\n",
				"                torch.save({\n",
				"                    'epoch': e,\n",
				"                    'model_state_dict': model.state_dict(),\n",
				"                    'optimizer_state_dict': optimizer.state_dict(),\n",
				"                    'val_acc': val_acc,\n",
				"                    'train_acc': train_acc,\n",
				"                    'train_loss': train_loss,\n",
				"                    'val_loss': val_loss,\n",
				"                }, checkpoint_path)\n",
				"            else:\n",
				"                patience_counter += 1\n",
				"\n",
				"            # Save last checkpoint every epoch\n",
				"            torch.save({\n",
				"                'epoch': e,\n",
				"                'model_state_dict': model.state_dict(),\n",
				"                'optimizer_state_dict': optimizer.state_dict(),\n",
				"                'val_acc': val_acc,\n",
				"                'train_acc': train_acc,\n",
				"                'train_loss': train_loss,\n",
				"                'val_loss': val_loss,\n",
				"            }, last_checkpoint_path)\n",
				"\n",
				"            # Early stopping\n",
				"            if patience_counter >= early_stop_patience:\n",
				"                print(f\"Early stopping at epoch {e+1} (no improvement for {early_stop_patience} epochs)\")\n",
				"                break\n",
				"\n",
				"    except KeyboardInterrupt:\n",
				"        print(\"Training interrupted. Saving last checkpoint...\")\n",
				"        torch.save({\n",
				"            'epoch': e,\n",
				"            'model_state_dict': model.state_dict(),\n",
				"            'optimizer_state_dict': optimizer.state_dict(),\n",
				"            'val_acc': val_acc,\n",
				"            'train_acc': train_acc,\n",
				"            'train_loss': train_loss,\n",
				"            'val_loss': val_loss,\n",
				"        }, last_checkpoint_path)\n",
				"\n",
				"    print(f\"Best Val Accuracy: {best_val_acc:.2f}% at epoch {best_epoch+1}\")\n",
				"    # prof_eval_base_acc = eval(model, test_base, base_classes, 128, device, label=\"Test Zero Shot on Base\")\n",
				"    # prof_eval_novel_acc = eval(model, test_novel, novel_classes, 128, device, label=\"Test Zero Shot on Novel\")\n",
				"\n",
				"    train_loss, train_acc = test_step(model, train_loader, cost_function, device=device, desc=\"Final Train Evaluation\")\n",
				"    val_loss, val_acc = test_step(model, val_loader, cost_function, device=device, desc=\"Final Valid Evaluation\")\n",
				"    eval_base_loss, eval_base_acc = test_step(model, test_loader_base, cost_function, device=device, desc=\"Final Base Evaluation\", categories=base_classes)\n",
				"    eval_novel_loss, eval_novel_acc = test_novel_step(model, test_loader_novel, cost_function, device=device, desc=\"Final Novel Evaluation\", categories=novel_classes)\n",
				"    hm = harmonic_mean(eval_base_acc, eval_novel_acc)\n",
				"\n",
				"    # Log final metrics\n",
				"    writer.add_scalar(\"Test/Base Accuracy\", eval_base_acc)\n",
				"    writer.add_scalar(\"Test/Novel Accuracy\", eval_novel_acc)\n",
				"    writer.add_scalar(\"Test/Harmonic Mean\", hm)\n",
				"    writer.flush()\n",
				"\n",
				"    print(\"After Training:\")\n",
				"    print(f\"Train Loss: {train_loss:.5f}\\t\\tTrain Accuracy: {train_acc:.5f}\")\n",
				"    print(f\"Valid Loss: {val_loss:.5f}\\t\\tValid Accuracy: {val_acc:.5f}\")\n",
				"    print(f\"Base Loss: {eval_base_loss:.5f}\\t\\tBase Accuracy: {eval_base_acc:.5f}\")\n",
				"    print(f\"Novel Loss: {eval_novel_loss:.5f}\\t\\tNovel Accuracy: {eval_novel_acc:.5f}\")\n",
				"\n",
				"    print(\"\\nResults:\")\n",
				"    print(f\"🔍 Base classes accuracy: {eval_base_acc:.2f}%\")\n",
				"    print(f\"🔍 Novel classes accuracy: {eval_novel_acc:.2f}%\")\n",
				"    print(f\"🔍 Harmonic Mean: {hm:.2f}%\")\n",
				"\n",
				"    # Plot accuracy and loss curves\n",
				"    plt.figure(figsize=(10,4))\n",
				"    plt.subplot(1,2,1)\n",
				"    plt.plot(range(epochs), train_losses, label=\"Train Loss\")\n",
				"    plt.plot(range(epochs), val_losses, label=\"Val Loss\")\n",
				"    plt.xlabel(\"Epoch\")\n",
				"    plt.ylabel(\"Loss\")\n",
				"    plt.title(\"Loss Curve\")\n",
				"    plt.legend()\n",
				"    plt.grid(True)\n",
				"\n",
				"    plt.subplot(1,2,2)\n",
				"    plt.plot(range(epochs), train_accs, label=\"Train Acc\")\n",
				"    plt.plot(range(epochs), val_accs, label=\"Val Acc\")\n",
				"    plt.xlabel(\"Epoch\")\n",
				"    plt.ylabel(\"Accuracy (%)\")\n",
				"    plt.title(\"Accuracy Curve\")\n",
				"    plt.legend()\n",
				"    plt.grid(True)\n",
				"\n",
				"    plt.tight_layout()\n",
				"    plt.savefig(f\"outputs/{run_name}_curves.png\")\n",
				"    plt.show()\n",
				"\n",
				"    # Log charts to TensorBoard\n",
				"    writer.add_figure(\"Curves\", plt.gcf())\n",
				"    writer.close()\n",
				"\n",
				"main_cocoop()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "u5HXX1FKghM8"
			},
			"source": [
				"## Evaluation: CoCoOp Baseline\n",
				"##### TODO\n",
				"- Evaluate CoCoOp on:\n",
				"    - Base test set (base classes)\n",
				"    - Novel test set (novel classes) — zero-shot generalization\n",
				"- Compute Harmonic Mean\n",
				"- Compare with baseline CLIP (from Section 2)\n",
				"- Show:\n",
				"    - Table with Base, Novel, HM for both CLIP and CoCoOp\n",
				"    - Confusion matrices\n",
				"    - Accuracy per class (bar plots)\n",
				"    - Line plot of train/val curves\n",
				"- Discuss: Where does CoCoOp help most? Are any flowers still confused?"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "fJ_zpWRkghM8"
			},
			"source": [
				"## Extensions"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "jJP0e3fwghM8"
			},
			"source": [
				"## Conclusion & Takeaways\n",
				"##### TODO\n",
				"- Summarize performance gains vs. zero-shot\n",
				"- Highlight where CoCoOp shines and its limitations\n",
				"- Discuss extensibility and future work\n",
				"- Reflect on challenges in fine-grained adaptation"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "sHvnymPyghM8"
			},
			"source": [
				"## References\n",
				"##### TODO\n",
				"- Radford et al., 2021 (CLIP)\n",
				"- Zhou et al., 2022 (CoOp/CoCoOp)\n",
				"- Additional relevant few-shot learning work (e.g., Tip-Adapter, VPT, etc.)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"id": "J3cLVIqMghM8"
			},
			"source": [
				"## Appendix"
			]
		}
	],
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"gpuType": "T4",
			"provenance": []
		},
		"kernelspec": {
			"display_name": "base",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.13"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
